{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0316e8-fbaf-485f-8a96-e1b94f1bdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import csv\n",
    "import configparser\n",
    "from census import Census\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781dcf59-e85f-4a64-8233-9d99e3ffbb3f",
   "metadata": {},
   "source": [
    "### FDIC API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058f0c82-c91a-4f61-a797-e8ed1141bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape fdic api and save it to a csv file \n",
    "def fdic_api(url, params, file_path, data_points: int): \n",
    "    \"\"\"\n",
    "    This function retrieves data from the FDIC API in batches, saving it to a CSV file. \n",
    "    It uses pagination by adjusting the 'offset' and 'limit' query parameters for each batch.\n",
    "\n",
    "    Parameters: \n",
    "    url - base url of the FDIC api\n",
    "    params - query parameters for the API (e.g. fields that needs to be included in the dataset being retrieved, format (csv, txt, json etc)\n",
    "    file_path - file path where retrieved file gets saved \n",
    "    data_points - total number of data points available for retrieval (found in the metadata)\n",
    "    \"\"\"\n",
    "    # define total number of fdic limit per API request\n",
    "    fdic_limit = 10000\n",
    "\n",
    "    # calculate number of batches to retrieve all data\n",
    "    total_data_pts = data_points\n",
    "    num_of_batches = (total_data_pts // fdic_limit) + (1 if total_data_pts % fdic_limit > 0 else 0) # if remainder is greater than 0, add 1 to the num batches, otherwise add 0\n",
    "\n",
    "    with open(file_path, 'w', newline = '', encoding = 'utf-8') as file: \n",
    "        writer = None\n",
    "\n",
    "        # update query parameters for each batch\n",
    "        for batch in range(num_of_batches): \n",
    "            params['offset'] = batch * fdic_limit \n",
    "            params['limit'] = fdic_limit # ensures the api only fetches 10000 data points at a time \n",
    "\n",
    "            # make the api request\n",
    "            response = requests.get(url, params)\n",
    "            if response.status_code == 200: \n",
    "                try: \n",
    "                    response_data = response.text.splitlines()\n",
    "                    reader = csv.reader(response_data) # parse the response data into rows using csv.reader\n",
    "                    # write data to file\n",
    "                    if writer is None: \n",
    "                        writer = csv.writer(file) # writes rows to the file\n",
    "                        writer.writerows(reader)\n",
    "                    else: \n",
    "                        next(reader) # skips header row for all other iterations\n",
    "                        writer.writerows(reader)\n",
    "                except Exception as e:\n",
    "                    print(f'An error occurred while reading batch {batch + 1}: {e}')\n",
    "                    raise\n",
    "            else: \n",
    "                print(f'Error: {response.status_code}')\n",
    "                print(f'{response.text}')\n",
    "                break\n",
    "    print(f'All data retrieved successfully. {total_data_pts} data points saved to {file_path}')\n",
    "                \n",
    "    # above function works if api has a limit (needs pagination), otherwise the function below works: \n",
    "    # response = requests.get(url, params)\n",
    "    # if response.status_code == 200:\n",
    "    #     try: \n",
    "    #         response_data = response.text.splitlines()\n",
    "    #         reader = csv.reader(response_data)\n",
    "    #         with open(file_path, 'w', newline = '', encoding = 'utf-8') as file: \n",
    "    #             writer = csv.writer(file)\n",
    "    #             writer.writerows(reader)\n",
    "    #     except Exception as e:\n",
    "    #         print(f'An error occurred while writing to the file: {e}')\n",
    "    #         raise\n",
    "    # else:\n",
    "    #     print(f'Error: {response.status_code}')\n",
    "    #     print(f'{response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28a0c55-1994-4bbd-9943-30c86320dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdic api has a limit of 10000 data points per request\n",
    "# institutions file - FDIC API\n",
    "\n",
    "# base_url_institutions = 'https://banks.data.fdic.gov/api/institutions?'\n",
    "# insti_params = {\n",
    "#     'fields': 'ACTIVE,ADDRESS,ADDRESS2,ASSET,BKCLASS,CBSA,CBSA_DIV,CBSA_DIV_FLG,CBSA_DIV_NO,CBSA_METRO,CBSA_METRO_FLG,CBSA_METRO_NAME,CBSA_MICRO_FLG,CBSA_NO,CITY,CLCODE,COUNTY,ENDEFYMD,ESTYMD,FED,FED_RSSD,INACTIVE,LATITUDE,LONGITUDE,NAME,NETINC,OFFDOM,OFFICES,OFFOA,STCNTY,STNAME,STNUM,UNINUM,WEBADDR,ZIP',\n",
    "#     'format': 'csv'\n",
    "# }\n",
    "# insti_file_path = 'institutions_data.csv'\n",
    "# insti_data_pts = 27825\n",
    "\n",
    "# fdic_api(base_url_institutions, insti_params, insti_file_path, insti_data_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f280f7-9adc-44a0-b2af-0f562bb9cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations file - FDIC API \n",
    "\n",
    "# base_url_locations = 'https://banks.data.fdic.gov/api/locations?'\n",
    "# loc_params = {\n",
    "#     'fields': 'ADDRESS,BKCLASS,CBSA,CBSA_DIV,CBSA_DIV_FLG,CBSA_DIV_NO,CBSA_METRO,CBSA_METRO_FLG,CBSA_METRO_NAME,CBSA_MICRO_FLG,CBSA_NO,CITY,COUNTY,ESTYMD,MAINOFF,NAME,OFFNAME,OFFNUM,SERVTYPE,STALP,STCNTY,STNAME,UNINUM,ZIP',\n",
    "#     'format': 'csv',\n",
    "#     'limit': 10000,\n",
    "#     'offset': 0\n",
    "# }\n",
    "# loc_file_path = 'locations_data.csv'\n",
    "# loc_data_pts = 78908\n",
    "\n",
    "# fdic_api(base_url_locations, loc_params, loc_file_path, loc_data_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7312f19b-2526-41d3-87b9-5ae865ccdcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failures (list of bank failures up to data) - FDIC API \n",
    "\n",
    "# base_url_failures = 'https://banks.data.fdic.gov/api/failures?'\n",
    "# fail_params = {\n",
    "#     'fields': 'NAME,CITYST,FAILDATE,FAILYR,CHCLASS1,RESDATE,RESTYPE,QBFDEP,QBFASSET,COST,PSTALP',\n",
    "#     'format': 'csv', \n",
    "#     'limit': 10000,\n",
    "#     'offset': 0\n",
    "# }\n",
    "# fail_file_path = 'failures_data.csv'\n",
    "# fail_data_pts = 4111\n",
    "\n",
    "# fdic_api(base_url_failures, fail_params, fail_file_path, fail_data_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d3fddc-eae9-4d22-ba71-5128ec3e7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics (summary of demographic information) - FDIC API \n",
    "# demographics filtered using CALLYM from Jan 2015 - Jan 2025\n",
    "\n",
    "# base_url_demographics = 'https://banks.data.fdic.gov/api/demographics?'\n",
    "# demo_params = {\n",
    "#     'filters': 'CALLYM:[\"201501\" TO \"202501\"]',\n",
    "#     'fields': 'ACTEVT,BRANCH,CALLYM,CALLYMD,CBSANAME,CERT,CLCODE,CMSA,CNTRYALP,CNTRYNUM,CNTYNUM,CSA,DIVISION,FDICAREA,METRO,MNRTYCDE,OFFDMULT,OFFTOT,OFFSTATE,WEBADDR',\n",
    "#     'format': 'csv',\n",
    "#     'limit': 10000,\n",
    "#     'offset': 0\n",
    "# }\n",
    "# demo_file_path = 'demographics_data.csv'\n",
    "# demo_data_pts = 190714\n",
    "\n",
    "# fdic_api(base_url_demographics, demo_params, demo_file_path, demo_data_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17c0b36-216e-4c46-a6b8-62f3ba2850cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FDIC Summary of Deposits \n",
    "\n",
    "# base_url_sod = 'https://banks.data.fdic.gov/api/sod?'\n",
    "# sod_params = {\n",
    "#     'filters': 'YEAR:[2024 TO 2025]',\n",
    "#     'fields': 'ADDRESSBR,ADDRESS,ASSET,BKCLASS,CERT,CITY,CITYBR,CNTRYNA,CNTRYNAB,CNTYNAMB,CNTYNUMB,DEPSUM,DEPSUMBR,NAMEBR,NAMEFULL,SIMS_LATITUDE,SIMS_LONGITUDE,STALP,STALPBR,STALPHCR,STCNTY,STCNTYBR,STNAME,STNAMEBR,STNUMBR,UNINUMBR,ZIP,ZIPBR',\n",
    "#     'format': 'csv',\n",
    "#     'limit': 10000, \n",
    "#     'offset': 0\n",
    "# }\n",
    "\n",
    "# sod_file_path = 'sod_data.csv'\n",
    "# sod_data_pts = 76742\n",
    "\n",
    "# fdic_api(base_url_sod, sod_params, sod_file_path, sod_data_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47daf160-1977-439e-8acc-1ef81e8c7c04",
   "metadata": {},
   "source": [
    "### US Census Bureau API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e077b42-265d-41a7-aafe-e63dc92e25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "api_key = config['USCensus']['api_key']\n",
    "c = Census(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a083bb-ffb7-4db7-b95e-6317cdeb9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACS 5 year estimate\n",
    "# Data Profilesâ€¯contain broad social, economic, housing, and demographic information. \n",
    "# The data are presented as estimates and percentages. \n",
    "# Data Profiles are available down to the census tract level.\n",
    "\n",
    "# acs5_dp_response = c.acs5.get(\n",
    "#     fields = acs5_dp_variables,\n",
    "#     geo = {\"for\": \"tract:*\"} # census tract level\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4d4eb-4e8d-4e3b-89ae-7aa7b48edce4",
   "metadata": {},
   "source": [
    "### Census Tract tigerWEB Shapefiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f384faa9-b874-4c65-b9ee-8fdae78505fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_shp_url = 'https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/Tracts_Blocks/MapServer/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900ae95-c18e-46e5-a06d-e893343b6d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d411aa-10d5-4112-96b9-3c15b1849551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ce1cebf-1f1c-4add-8bbb-34513d9618ba",
   "metadata": {},
   "source": [
    "### Geocoding addresses\n",
    "- collecting data (geocoding addresses) from the US Census Bureau's Geocoding services web API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ef8fb6-f567-4591-a96b-8879edf2eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to batch geocode \n",
    "def geocode(url, params, input_file, output_file):\n",
    "    with open(input_file, 'rb') as file: \n",
    "        files = {'addressFile': file}\n",
    "        response = requests.post(url = url, params = params, files = files)\n",
    "        if response.status_code == 200:\n",
    "            try: \n",
    "                with open(output_file, 'wb') as output: \n",
    "                    output.write(response.content)\n",
    "                print(f'Geocoded results saved to {output_file}')\n",
    "            except Exception as e: \n",
    "                print(f'An error ocurred while geocoding: {e}')\n",
    "        else: \n",
    "            print(f'Error: {response.status_code}, {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42a9bcee-48bc-43a8-8af3-456951da9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_geocode_url = 'https://geocoding.geo.census.gov/geocoder/geographies/addressbatch'\n",
    "\n",
    "geocode_params = {\n",
    "    'returntype': 'geographies',\n",
    "    'benchmark': 'Public_AR_Current',\n",
    "    'vintage': 'Current_Current'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "075c5e5f-f2e0-4811-bf1e-abb048e4c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoded results saved to addresses_geocoded_1.csv\n",
      "Geocoded results saved to addresses_geocoded_2.csv\n",
      "Geocoded results saved to addresses_geocoded_3.csv\n",
      "Geocoded results saved to addresses_geocoded_4.csv\n",
      "Geocoded results saved to addresses_geocoded_5.csv\n",
      "Geocoded results saved to addresses_geocoded_6.csv\n",
      "Geocoded results saved to addresses_geocoded_7.csv\n",
      "Geocoded results saved to addresses_geocoded_8.csv\n",
      "Geocoded results saved to addresses_geocoded_9.csv\n"
     ]
    }
   ],
   "source": [
    "# create a loop for all the nine address csv file that needs to be geocoded \n",
    "# geocoding also filtered the locations data - there was NO MATCH for US owned bank branches outside of the US \n",
    "# all geocoded addresses must be concatenated in one csv file \n",
    "\n",
    "# for i in range(1, 10):\n",
    "#     geocode_input_file = f'addresses_batch_{i}.csv'\n",
    "#     geocode_output_file = f'addresses_geocoded_{i}.csv'\n",
    "#     geocode(batch_geocode_url, geocode_params, geocode_input_file, geocode_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283f201-39c2-4a37-ab44-0f894111b4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be2c24d9-0883-42ac-8439-c1ace1d99384",
   "metadata": {},
   "source": [
    "### TIGERline Shapefiles for Census Tracts\n",
    "- Use Beautiful Soup to parse through the HTML and download all the TIGER/line shapefiles\n",
    "- Can also cartographic shapefiles if precision in boundary is not a major feature in the analysis. This link contains the 2022 cartographic boundary for Census Tracts in the US: https://catalog.data.gov/dataset/2022-cartographic-boundary-file-shp-current-census-tract-for-united-states-1-500000\n",
    "- Otherwise, it's better to use the TIGER/line shapefile for more detailed geographic information. This link contains the 2024 TIGER/line shapefiles for Census Tracts provided by the US Census Bureau: https://www2.census.gov/geo/tiger/TIGER2024/TRACT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c3049-f2e6-4df3-b3e6-a1c4662de351",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiger_shp_url = 'https://www2.census.gov/geo/tiger/TIGER2024/TRACT/'\n",
    "\n",
    "# send a request and parse the webpage using beautifulsoup\n",
    "response = requests.get(tiger_shp_url)\n",
    "bsoup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# create a folder to save the downloaded files\n",
    "os.makedirs('shapefiles', exist_ok = True)\n",
    "\n",
    "# loop through all the anchor tags with href \n",
    "def find_shp(url): \n",
    "    for i, tr_tag in enumerate(bsoup.find_all('tr')):\n",
    "        if i < 3:\n",
    "            continue\n",
    "\n",
    "        for anchor in tr_tag.find_all('a', href = True):\n",
    "            shp_name = anchor['href']\n",
    "            if shp_name.endswith('.zip'):\n",
    "                shp_url = url + shp_name\n",
    "                shp_path = os.path.join('shapefiles', shp_name)\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
