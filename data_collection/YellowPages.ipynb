{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e6b3d5-d3d6-4d7e-8968-86fb91112e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cb696-d412-4809-823d-8ca78b7e50f9",
   "metadata": {},
   "source": [
    "# Scrape data from Yellow Pages\n",
    "\n",
    "Yellow Pages contains business information and can be used to find bank branches by ZIP Code. We did not end up using this script in the analysis as the run time took too long."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3977f8d-f6ff-45b2-932c-8e9770e68075",
   "metadata": {},
   "source": [
    "## Get ZIP Codes\n",
    "\n",
    "In this script, we'll use the Census Bureau ZCTA to County Relationship [File](https://www2.census.gov/geo/docs/maps-data/data/rel2020/zcta520/tab20_zcta520_county20_natl.txt) to get a list of national ZIP Codes.\n",
    "\n",
    "Note: The Census County Relationship file lists 46,960 ZCTAs (ZIP Code equivalents). Scraping business data from Yellow Pages for a particular ZIP code takes between 5-10 seconds. Thus, scraping this data for all ZIP codes in the U.S. will take anywhere from 2 to 5 days. For this reason, we chose not to proceed with this method of data gathering for our analysis. Instead, we have scraped data for Philadelphia County for illustrative purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ece1ed-f5ee-443a-92eb-b475e4368bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46960 ZIP Codes listed nationally in the Census Bureau ZCTA to County Relationship file.\n",
      "\n",
      "We will scrape data for the 49 ZIP Codes listed in Philadelphia County.\n"
     ]
    }
   ],
   "source": [
    "zip_county = pd.read_csv('https://www2.census.gov/geo/docs/maps-data/data/rel2020/zcta520/tab20_zcta520_county20_natl.txt', sep='|')\n",
    "# print(zip_county.sample(5))\n",
    "\n",
    "# drop counties without zip codes\n",
    "zip_county.dropna(inplace = True)\n",
    "\n",
    "# create list of zips\n",
    "zips = zip_county.loc[:,\"GEOID_ZCTA5_20\"].astype(str).str[:-2].str.zfill(5).tolist()\n",
    "print(f\"There are {len(zips)} ZIP Codes listed nationally in the Census Bureau ZCTA to County Relationship file.\\n\")\n",
    "\n",
    "# create a list of zips in Philadelphia County\n",
    "zips_Philly = zip_county.loc[zip_county['NAMELSAD_COUNTY_20'] == 'Philadelphia County']\n",
    "zips_Philly = zips_Philly.loc[:,\"GEOID_ZCTA5_20\"].astype(str).str[:-2].str.zfill(5).tolist()\n",
    "print(f\"We will scrape data for the {len(zips_Philly)} ZIP Codes listed in Philadelphia County.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bb8ff11-e48e-4d80-9561-62f414aa9b4b",
   "metadata": {},
   "source": [
    "## Define function to scrape Yellow Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d3697c-4dc6-4992-99ef-b7ebc0eb5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeYellowPages(search_term: str, search_zip: str, search_page: int = 1):\n",
    "    '''\n",
    "        Function scrapes business data from yellowpages.com\n",
    "\n",
    "        Args:\n",
    "            search_term (str): type of business to search in 'Find a business' bar\n",
    "            search_zip (str): ZIP code to search in 'Where?' bar\n",
    "            search_page (int): results page to scrape; default 1\n",
    "\n",
    "        Return:\n",
    "            business_df: Dataframe populated with results from your search ZIP Code\n",
    "                with the following fields:\n",
    "                    - Name\n",
    "                    - Address\n",
    "                    - City\n",
    "                    - State\n",
    "                    - Zip\n",
    "            check_next_page (bool)\n",
    "            \n",
    "    '''\n",
    "    # base url\n",
    "    url = 'https://www.yellowpages.com/search'\n",
    "\n",
    "    # search parameters\n",
    "    params = {\n",
    "        \"search_terms\": search_term,\n",
    "        \"geo_location_terms\": search_zip,\n",
    "        \"page\": search_page\n",
    "    }\n",
    "\n",
    "    # create df to hold results\n",
    "    business_df = pd.DataFrame(columns = ['Name', 'Address', 'City', 'State', 'Zip'])\n",
    "    \n",
    "    # flag to determine whether next page should be checked for additional results\n",
    "    check_next_page = 1\n",
    "\n",
    "    # flag for error in search\n",
    "    error_flag = 0\n",
    "    \n",
    "    try:\n",
    "        # request page\n",
    "        page = requests.get(url, params = params)\n",
    "    \n",
    "        # parse html\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "        # find results on webpage\n",
    "        search_results_organic = soup.find(\"div\", class_=\"search-results organic\")\n",
    "        results = search_results_organic.find_all(\"div\", class_ = \"result\")    \n",
    "    \n",
    "        # iterate through results\n",
    "        for result in results:\n",
    "            business_name = result.find(\"a\", class_ = 'business-name').text\n",
    "            street_address = None\n",
    "            city = None\n",
    "            state = None\n",
    "            zipcode = None\n",
    "        \n",
    "            # skip ATMS\n",
    "            if 'ATM' in business_name:\n",
    "               continue\n",
    "        \n",
    "            # if street address exists, extract data\n",
    "            if result.find(\"div\", class_ = 'street-address'):\n",
    "                street_address = result.find(\"div\", class_ = 'street-address').text\n",
    "        \n",
    "            # if locality exists, extract data        \n",
    "            if result.find(\"div\", class_ = 'locality'):\n",
    "                locality = result.find(\"div\", class_ = 'locality').text\n",
    "                city  = locality.split(',')[0]\n",
    "                state_zip = locality.split(',')[1]\n",
    "                state_zip = state_zip.strip()\n",
    "                state = state_zip.split(' ')[0]\n",
    "                zipcode = state_zip.split(' ')[1]\n",
    "       \n",
    "            # determine whether business is in correct geography\n",
    "            if search_zip == zipcode:\n",
    "                business_df.loc[len(business_df)] = [business_name, street_address, city, state, zipcode]\n",
    "                check_next_page = 1\n",
    "            else:\n",
    "                check_next_page = 0\n",
    "    except:\n",
    "        error_flag = 1\n",
    "    \n",
    "    return business_df, check_next_page, error_flag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dfa5776-371a-4f0c-abc1-c84ecfa5453b",
   "metadata": {},
   "source": [
    "## Scrape data for each ZIP Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c02df63-0d6a-4dba-9869-867517a76503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df to hold results\n",
    "results_df = pd.DataFrame(columns = ['Name', 'Address', 'City', 'State', 'Zip'])\n",
    "\n",
    "# list to hold misssing zips\n",
    "errored_zips = []\n",
    "\n",
    "# iterate through all zips\n",
    "for z in zips_Philly: \n",
    "    # initialize flags\n",
    "    check_next_page = 1\n",
    "    search_page = 1\n",
    "    error_flag = 0\n",
    "    \n",
    "    while check_next_page == 1:\n",
    "        # capture results\n",
    "        df, check_next_page, error_flag = scrapeYellowPages('banks', z, search_page)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # append df to results_df\n",
    "        results_df = pd.concat([results_df, df], ignore_index = True)\n",
    "\n",
    "        # track errored zips\n",
    "        if error_flag:\n",
    "            errored_zips.append(z)\n",
    "        \n",
    "        # increment search page\n",
    "        search_page += 1\n",
    "\n",
    "\n",
    "print(f'{len(results_df)} banks found in {len(zips_Philly)} zip codes.')      \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "546d6b36-9ef8-48cb-9e2f-ce81740140c9",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f817c-dc8b-4b14-8bad-1812a2d5db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "results_df.to_csv(\"data/Banks_Philadelphia.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
